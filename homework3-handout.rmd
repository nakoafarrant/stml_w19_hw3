---
title: "Homework 3 - 131/231"
date: "__Due on Friday March 1, 2019 at 11:59 pm__"
output: pdf_document
urlcolor: blue
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, 
                      cache=FALSE, 
                      fig.align='center')
indent1 = '    '      
indent2 = paste(rep(indent1, 2), collapse='')
```

For this homework you will need use the following packages. 

```{r, message=FALSE, warning=FALSE}

library(tidyverse)
library(ROCR)
library(tree)
library(maptree)
library(class)
library(lattice)
library(ggridges)
library(superheat)
library(kableExtra)
```

# Analyzing drug use

The first half of this homework involves the analysis of drug use.  The data set includes a total of 1885 observations on 32 variables. A detailed description of the data set can be found [here](https://archive.ics.uci.edu/ml/datasets/Drug+consumption+%28quantified%29#). For each observation, 12 attributes are known:

- ID: number of record in original database. Used for reference only.
- Age: Age of the participant
- Gender: Gender of the participant (M/F)
- Education: Level of education of the participant
- Country: Country of current residence of the participant
- Ethnicity: Ethnicity of the participant

Many of the covariates  have been transformed: some ordinal or categorical variables have been given numeric codes.  Part of this problem will involve appropriately re-transforming these variables.  The data also contains the following personality measurements:

- Nscore: NEO- FFI- R Neuroticism (Ranging from 12 to 60)
- Escore: NEO- FFI- R Extraversion (Ranging from 16 to 59)
- Oscore: NEO- FFI- R Openness (Ranging from 24 to 60)
- Ascore: NEO- FFI- R Agreeableness (Ranging from 12 to 60)
- Cscore: NEO- FFI- R Conscientiousness (Ranging from 17 to 59)
- Impulsive: Impulsiveness measured by BIS- 11
- SS: Sensation Seeking measured by ImpSS

Finally, participants were questioned concerning their use of 18 legal and illegal drugs (alcohol, amphetamines, amyl nitrite, benzodiazepine, cannabis, chocolate, cocaine, caffeine, crack, ecstasy, heroin, ketamine, legal highs, LSD, methadone, mushrooms, nicotine and volatile substance abuse) and one fictitious drug (Semeron) which was introduced to identify over-claimers. All of the drugs use the class system of CL0-CL6: CL0 = "Never Used", CL1 = "Used over a decade ago", CL2 = "Used in last decade", CL3 = "Used in last year", CL4 = "Used in last month", CL5 = "Used in last week", CL6 = "Used in last day".

```{r, warning=FALSE, message=FALSE}
drug_use <- read_csv('drug.csv', 
                   col_names = c('ID','Age','Gender','Education','Country','Ethnicity',
                                 'Nscore','Escore','Oscore','Ascore','Cscore','Impulsive',
                                'SS','Alcohol','Amphet','Amyl','Benzos','Caff','Cannabis',
                                'Choc','Coke','Crack','Ecstasy','Heroin','Ketamine',
                              'Legalh','LSD','Meth','Mushrooms','Nicotine','Semer','VSA'))
```

## 1. Logistic regression for drug use prediction
This problem has 3 parts for 131 students and 4 parts for 231 students.  As mentioned, the data uses some strange encodings for variables.  For instance, you may notice that the gender variable has type `double`. Here the value -0.48246 means male and 0.48246 means female.  Age was recorded at a set of categories but rescaled to a mean 0 numeric variable (we will leave that variable as is).  Similarly education is a scaled numeric quantity (we will also leave this variable as is).  We will however, start by transforming gender, ethnicity, and country to factors, and the drug response variables as ordered factors:


```{r}
drug_use <- drug_use %>% mutate_at(as.ordered, .vars=vars(Alcohol:VSA))
drug_use <- drug_use %>%
  mutate(Gender = factor(Gender, labels=c("Male", "Female"))) %>%
  mutate(Ethnicity = factor(Ethnicity, labels=c("Black", "Asian", "White", 
                                                "Mixed:White/Black", "Other", 
                                                "Mixed:White/Asian", 
                                                "Mixed:Black/Asian"))) %>%
  mutate(Country = factor(Country, labels=c("Australia", "Canada", "New Zealand", 
                                            "Other", "Ireland", "UK", "USA")))
```

__(a)__.  Define a new factor response variable `recent_cannabis_use` which is "Yes" if a person has used cannabis within a year, and "No" otherwise.  This can be done by checking if the `Cannabis` variable is _greater than or equal_ to `CL3`. Hint: use `mutate` with the `ifelse` command.  When creating the new factor set `levels` argument to `levels=c("No", "Yes")` (in that order). 

      
```{r, hide=TRUE}
drug_use <- drug_use %>%
  mutate(recent_cannabis_use=factor(ifelse(Cannabis >= "CL3", "Yes", "No"),
                                    levels=c("No", "Yes")))
```

__(b).__ We will create a new tibble that includes a subset of the original variables.  We will focus on all variables between `age` and `SS` as well as the new factor related to recent cannabis use.  Create `drug_use_subset` with the command:

```{r}
drug_use_subset <- drug_use %>% select(Age:SS, recent_cannabis_use)
```

Split `drug_use_subset` into a training data set and a test data set called `drug_use_train` and `drug_use_test`.  The training data should include 1500 randomly sampled observation and the test data should include the remaining observations in `drug_use_subset`. Verify that the data sets are of the right size by printing `dim(drug_use_train)` and `dim(drug_use_test)`. 


```{r train_test2, hide=TRUE}
train_index <- sample(nrow(drug_use_subset), 1500)

drug_use_train <- drug_use_subset[train_index,]
drug_use_test <- drug_use_subset[-train_index, ]

dim(drug_use_train)
dim(drug_use_test)
```
      
__(c).__  Fit a logistic regression to model `recent_cannabis_use` as a function of all other predictors in `drug_use_train`.  Fit this regression using the training data only.  Display the results by calling the `summary` function on the logistic regression object.


```{r}
logistic.cannabis =glm(recent_cannabis_use~., data=drug_use_train, family =binomial)

summary(logistic.cannabis)
```



__(d).__ (__231__ only).  Generalized linear models for binary data involve a link function, $g$ which relates a linear function of the predictors to a function of the probability $p$: $g(p) = \beta_0 + \beta_1X_1 + ... \beta_p X_p$.  $g$ is a function which maps $p \in [0, 1]$ to $\mathbb{R}$.  Logistic regression is based on the _logit_ link function, $g(p) = log(p/(1-p))$.  In class we mentioned another link function, called the probit: $g(p) = \Phi^{-1}(p)$ where $\Phi$ is the cumulative density function of the normal distribution. Another often used link function is the "c-log-log" link: $g(p) = log(-log(1-p))$.  

Plot the fitted values for logistic regression fit of the training data on the x-axis and the fitted values for the probit regression on y-axis.  In the plot command (assuming you use the base plotting package, not ggplot) set `pch=19` and `cex=0.2` (this makes the points smaller and more legible).   Include the line y=x with the command `abline(a=0, b=1, col="red")`.  Make another identical plot, this time replacing the y-axis with the predicted values from a cloglog 

```{r}
probit.cannabis = glm(recent_cannabis_use~., data=drug_use_train, family = binomial(link = "probit"))
cloglog.cannabis = glm(recent_cannabis_use~., data=drug_use_train, family = binomial(link = "cloglog"))

plot(logistic.cannabis[["fitted.values"]], probit.cannabis[["fitted.values"]], pch = 19, cex = 0.2, xlab = "Fitted Values from Logit", ylab = "Fitted Values from Probit")
abline(a=0, b=1, col="red")

```

```{r}
plot(logistic.cannabis[["fitted.values"]], cloglog.cannabis[["fitted.values"]], pch = 19, cex = 0.2, xlab = "Fitted Values from Logit", ylab = "Fitted Values from cloglog")
abline(a=0, b=1, col="red")
```


Comment on the differences between the estimated probabilities in each plot.  Things you should comment on include: 1) which link function (probit or cloglog) leads to predictions that are most similar to the logistic regression predictions? 2) for what range of probabilities are the probit and cloglog predictions values more or less extreme than the logit values? 3) Does either probit or cloglog regression seem to estimate systematically smaller or larger probabilities than the logistic regression for a certain range of probabilities?

The probit function leads to predictions that are more similar to the logistic predictions because the fitted values most closely follow the line y = x. The cloglog fitted vales are more extreme than the logit function for probablities of roughly 0.3 and below and they are less extreme than the logit function for probabilities of about 0.5 to 0.9. As a result, the cloglog seems to systematically estimate smaller or larger probabilities over these certain probability ranges. The probit function tends to have slightly higher values than the logit function for the range of probabilities from 0.3 until close to 0.5 and tends to estimate lower values than the logit function for the range from 0.6 until close to 0.9. These differences between the probit and logit are much less pronounced than in the cloglog vs logit comparison. Due to this minimal difference it feels a little strange to call these differences between probit and logit "systematic" but maybe this description is still fair given the consistently higher or lower points.


Hint: in logistic regression we set `family=binomial(link="logit"")`.  To fit probit and cloglog regressions change the value of the `link` argument appropriately.



## 2. Decision tree models of drug use

This problem has 3 parts for all students.

Construct a decision tree to predict `recent_cannabis_use` using all other predictors in `drug_use_train`.  Set the value of the argument `control = tree_parameters` where `tree_parameters` are:

```{r}
tree_parameters = tree.control(nobs=nrow(drug_use_train), minsize=10, mindev=1e-3)
```

This sets the smallest number of allowed observations in each leaf node to 10 and requires a deviance of at least 1e-3 to split a node.

```{r}
cannabis.tree = tree(recent_cannabis_use~.,data = drug_use_train, control = tree_parameters)
summary(cannabis.tree)
```


        
__(a).__ Use 10-fold CV to select the a tree which minimizes the cross-validation misclassification  rate.  Use the function `cv.tree`, and set the argument `FUN=prune.misclass`.  Note: you do not need to use a `do.chunk` function since the `tree` package will do cross validation for you.  Find the size of the tree which minimizes the cross validation error.  If multiple trees have the same minimum cross validated misclassification rate, set `best_size` to the smallest tree size with that minimum rate. 


```{r}

nfold = 10
set.seed(1)
folds = seq.int(nrow(drug_use_train)) %>%       ## sequential obs ids
    cut(breaks = nfold, labels=FALSE) %>%   ## sequential fold ids
    sample                                  ## random fold ids

cannabis.tree.cv = cv.tree(cannabis.tree, FUN = prune.misclass, K = 10, rand = folds)

best.cv = min(cannabis.tree.cv$size[which(cannabis.tree.cv$dev==min(cannabis.tree.cv$dev, na.rm = TRUE))])

cannabis.tree.cv$size
cannabis.tree.cv$dev

best.cv
```
The tree that minimizes the cross validation error has `r best.cv` nodes.
        
__(b).__ Prune the tree to the size found in the previous part and plot the tree using the `draw.tree` function from the `maptree` package. Set `nodeinfo=TRUE`.  Which variable is split first in this decision tree? 

```{r}
pruned.cannabis.tree = prune.tree(tree = cannabis.tree, best = best.cv)

draw.tree(tree = pruned.cannabis.tree, size = 2, cex = .3, nodeinfo=TRUE)
```


__(c).__ Compute and print the confusion matrix for the _test_ data using the function `table(truth, predictions)` where `truth` and `predictions` are the true classes and the predicted classes from the tree model respectively.  Note: when generated the predicted classes for the test data, set `type="class"` in the `predict` function. Calculate the true positive rate (TPR) and false positive rate (FPR) for the confusion matrix.  Show how you arrived at your answer.

```{r}

pruned.predict.test = predict(pruned.cannabis.tree, drug_use_test, 
                              type = "class")
testtable = table(pruned.predict.test, drug_use_test$recent_cannabis_use)

testtable

tpr = testtable[2,2]/(testtable[2,2] + testtable[1,2])
fpr = testtable[2,1]/(testtable[2,1] + testtable[1,1])

tpr
fpr
```

The true positive rate is `r tpr` and the false positive rate is `r fpr`. TPR = TP/(TP + FN) and FPR = FP/(FP + TN)
      
##  3. Model Comparison

This problem has 2 parts for all students. 

__(a).__ Plot the ROC curves for both the logistic regression fit and the decision tree on the same plot.  Use `drug_use_test` to compute the ROC curves for both the logistic regression model and the best pruned tree model.
```{r}
pruned.predict.test = predict(pruned.cannabis.tree, drug_use_test,  type = "vector")
pred.tree <- prediction(pruned.predict.test[,2], drug_use_test$recent_cannabis_use)
perf.tree <- performance(pred.tree, "tpr", "fpr")

logistic.probs.test = predict(logistic.cannabis, drug_use_test, type="response")
pred.logistic = prediction(logistic.probs.test, drug_use_test$recent_cannabis_use)
perf.logistic <- performance(pred.logistic, "tpr", "fpr")

perf.tree@x.name = perf.logistic@x.name= "False Positive Rate"
perf.tree@y.name = perf.logistic@y.name= "True Positive Rate"

# plot the two ROC curves on the same plot
plot(perf.tree, type = "l", col = "red")
par(new=TRUE)
plot(perf.logistic, type="l", col = "blue")

legend("bottomright", legend=c("Tree", "Logistic"), col=c("red", "blue"), lty=1:2, cex=0.8)
title("ROC", cex = 0.8)
```

      
__(b).__ Compute the AUC for both models and print them.  Which model has larger AUC?
```{r}
# Calculate AUC
auc.tree = performance(pred.tree, "auc")@y.values
auc.logistic = performance(pred.logistic, "auc")@y.values

auc.tree
auc.logistic
```
The AUC for the tree is `r auc.tree` which is smaller than the AUC from the logistic model which is `r auc.logistic`.

# 4. Clustering and dimension reduction for gene expression data

This problem involves the analysis of gene expression data from 327 subjects from Yeoh _et al_ (2002). The data set includes abundance levels for 3141 genes and a class label indicating one of 7 leukemia subtypes the patient was diagnosed with.   The paper describing their analysis of this data can be found [here](http://www.sciencedirect.com/science/article/pii/S1535610802000326). Read in the csv data in  `leukemia_data.csv`.  It is posted on Piazza in the resources tab with the homework:


```{r, results="hide", message=FALSE, warning=FALSE}
leukemia_data <- read_csv("leukemia_data.csv")
```



__(a).__ The class of the first column of `leukemia_data`, `Type`, is set to `character` by default. Convert the `Type` column to a factor using the `mutate` function.  Use the `table` command to print the number of patients with each leukemia subtype.  Which leukemia subtype occurs the least in this data? 


```{r}
leukemia_data_factor <- leukemia_data %>%
  mutate(Type = factor(Type))

type_table = table(leukemia_data_factor$Type)

```

```{r}
kable(type_table, "latex", booktabs = T, 
      caption = "Number of Patients with Each Sub-Type of Leukemia", digits = 4) %>%  
      kable_styling(bootstrap_options = "striped", full_width = F, position = "center")
```

The BCR-ABL sub-type of leukemia has the fewest patients in this data set at 15 individuals.

__(b).__ Run PCA on the leukemia data using `prcomp` function with `scale=TRUE` and `center=TRUE` (this scales each gene to have mean 0 and variance 1).  Make sure you exclude the `Type` column when you run the PCA function (we are only interested in reducing the dimension of the gene expression values and PCA doesn't work with categorical data anyway).  Plot the proportion of variance explained by each principal component (PVE) and the cumulative PVE side-by-side.



```{r}

# Standardize the variables by subtracting mean and divided by standard deviation
scale.leukemia = scale(leukemia_data_factor[,-c(1)], center=TRUE, scale=TRUE)

pr.out = prcomp(scale.leukemia, scale =TRUE, center=TRUE)

pr.var = pr.out$sdev^2 # variance is the square of the standard deviation of the PCA output

pve <- pr.var/sum(pr.var)
cumulative_pve <- cumsum(pve)

## This will put the next two plots side by side    
par(mfrow=c(1, 2))

## Plot proportion of variance explained
plot(pve, type="l", lwd=3)
plot(cumulative_pve, type="l", lwd=3)
```

    
__(c).__  Use the results of PCA to project the data into the first two principal component dimensions. `prcomp` returns this dimension reduced data in the first columns of `x`.  Plot the data as a scatter plot using `plot` function with `col=plot_colors` where `plot_colors` is defined 


```{r, echo=TRUE}
rainbow_colors <- rainbow(7)
plot_colors <- rainbow_colors[leukemia_data_factor$Type]
```

This will color the points according to the leukemia subtype.  Add the leukemia type labels to the plot using `text` with `labels` argument set to the leukemia type and the col to `plot_colors` (it may help legibility to make the points on the plot very small by setting `cex` to a small number). Which group is most clearly separated from the others along the PC1 axis?  Which genes have the highest absolute loadings for PC1 (the genes that have the largest weights in the weighted average used to create the new variable PC1)?  You can find these by taking the absolute values of the first principal component loadings and sorting them.  Print the first 6 genes in this sorted vector using the `head` function. 

```{r}
par(mfrow=c(1, 1))

PC1 = pr.out$x[,1]
PC2 = pr.out$x[,2]
plot(PC1, PC2, col = plot_colors, pch =19, xlab ="Z1",ylab="Z2", cex = 0.1)
text(pr.out$x[,c(1,2)], labels=(leukemia_data_factor$Type), cex = 0.25)

abs_pc1 = order(abs(PC1))
head(leukemia_data_factor$Type[abs_pc1], 6)

```

T-ALL seems to be most clearly separated from the other groups on the PC1 axis based on the figure. The  top 6 genes that seem to have the highest absolute loadings for PC1 are: TEL-AML1, Hyperdip50, BCR-ABL, E2A-PBX1, Hyperdip50, and OTHERS.

__(d).__ (__231 Only__) PCA orders the principal components according to the amount of total variation in the data that they explain.  This does not mean, however, that the principal components are sorted in terms of how useful they are at capturing variation between the leukemia groups. For example, if gene expression varied significantly with age and gender (independent of leukemia status), the first principal components could reflect genetic variation due to age and gender, but not to leukemia.  The first scatter plot shows that the second PC is not a good discriminator of leukemia type.  See if the 3rd PC is better at discriminating between leukemia types by plotting the data projected onto the _first_ and _third_ principal components (not the second).  

```{r}
par(mfrow=c(1, 1))

PC3 = pr.out$x[,3]
plot(PC1, PC3, col = plot_colors, pch =19, xlab ="Z1",ylab="Z3", cex = 0.1)
text(pr.out$x[,c(1,3)], labels=(leukemia_data_factor$Type), cex = 0.25)
```

This second scatter with the first and third principal components does a better job of separating the different genes as compared to the first scatter plot with the PC1 and PC2. In this second scatter, the separation of T-ALL has slightly worsened than in the previous scatter. However, T-ALL is still fairly well separated in the second scatter. Additionally there is better separation of E2A-PBX1, Hyperdip50, and TEL-AML1 genes in this second scatter. As a result, PC3 seems to be better at discriminating leukemia types than PC2.

__(e.)__  (__231 Only__)  For this part we will be using the `ggridges` library.  Create a new tibble where the first column (call it `z1`) is the projection of the data onto the first principal component and the second column is the leukemia subtype (`Type`).  Use `ggplot` with `geom_density_ridges` to create multiple stacked density plots of the projected gene expression data.  Set the ggplot aesthetics to `aes(x = z1, y = Type, fill = Type)`.  Make another identical plot, except replace `z1` with `z3`, the projection of the data onto the third principal component. Identify two leukemia subtypes that are nearly indistinguishable when the gene expression data is projected onto the first PC direction, but easily distinguishable when projected onto the third PC direction.   

```{r}

pca_df = data.frame("z1" = PC1, "Type" = leukemia_data_factor$Type)
pca_tibble = as.tibble(pca_df)

plot.new()
ggridge_plot_z1 = ggplot(pca_tibble, aes(x=z1, y = Type, fill = Type)) + 
  geom_density_ridges() +
  title("Projection of Leukemia Type Data on PC1 from PCA") +
  labs(x = "Z1", y = "Leukemia Type") +
  theme_classic()

ggridge_plot_z1

```

```{r}
plot.new()
pca_df_z3 = data.frame("z3" = PC3, "Type" = leukemia_data_factor$Type)
pca_tibble_z3 = as.tibble(pca_df_z3)

ggridge_plot_z3 = ggplot(pca_tibble_z3, aes(x=z3, y = Type, fill = Type)) + 
  geom_density_ridges() +
  title("Projection of Leukemia Type Data on PC3 from PCA") +
  labs(x = "Z3", y = "Leukemia Type") +
  theme_classic()

ggridge_plot_z3
```


E2A-PBX1 and the OTHERS genes are fairly indistinguishable when projected onto the first principal component, but when the data is projected on the third principal component, these two gene categories are fairly distinct from each other. Hyperdip50 and BCR-ABL are also fairly similar when projected on the first principal component, and while there some distinguishing attributes for the densities of these two curves when they're projected on the third principal component, it isn't clear that this projection on the third component is sufficient to distinguish these two gene groups.

__(f.)__ Use the `filter` command to create a new tibble `leukemia_subset` by subsetting to include only rows for which `Type` is either T-ALL, TEL-AML1, or Hyperdip50.  Compute a euclidean distance matrix between the subjects using the `dist` function and then run hierarchical clustering using complete linkage. Plot two dendrograms based on the hierarchical clustering result. In the first plot, force 3 leukemia types to be the labels of terminal nodes, color the branches and labels to have 3 groups and rotate the dendrogram counter-clockwise to have all the terminal nodes on the right. In the second plot, do all the same things except that this time color all the branches and labels to have 5 groups. Please make sure library `dendextend` is installed. Hint: `as.dendrogram`, `set_labels`, `color_branches`, `color_labels` and `plot(..., horiz = TRUE)` may be useful.

```{r}
library(dendextend)
```


```{r fig.align="center", fig.height=8, fig.width=5}
leukemia_subset <- leukemia_data_factor %>%
  filter(Type == "T-ALL" | Type == "TEL-AML1" | Type == "Hyperdip50") 

# do we need to re-scale the subset before computing the distance matrix?
dis  <- dist(scale(leukemia_subset[,-c(1)], center=TRUE, scale=TRUE), method = "euclidean")

set.seed(1)
leuk_hc = hclust(dis, method = "complete")
  
leukemia_dend3 = leuk_hc %>% 
  as.dendrogram() %>% 
  color_branches(k = 3) %>%  
  color_labels(k = 3)

leukemia_dend3 = set_labels(leukemia_dend3, labels=leukemia_subset$Type[order.dendrogram(leukemia_dend3)]) 
leukemia_dend3= set(leukemia_dend3, "labels_cex", 0.15)
plot(leukemia_dend3, horiz = T)


```


```{r fig.align="center", fig.height=8, fig.width=5}

leukemia_dend5 = leuk_hc %>% 
  as.dendrogram() %>% 
  color_branches(k = 5) %>%  
  color_labels(k = 5)

leukemia_dend5 = set_labels(leukemia_dend5, labels=leukemia_subset$Type[order.dendrogram(leukemia_dend5)]) 
leukemia_dend5 = set(leukemia_dend5, "labels_cex", 0.15)
plot(leukemia_dend5, horiz = T)
```


__(g).__  (__231 only__).  Use `superheat` to plot the distance matrix from the part above.  Order the rows and columns by the hierarchical clustering you obtained in the previous part. You should see a matrix with a _block diagonal_ structure. The labels (corresponding to leukemia types) will not be available to read on the plot.  Print them out by looking at `leukemia_subset$Type` ordered by clustering order.  Based on this plot which two leukemia types (of the three in the subset) seem more similar to one another? Hint: use `heat.pal = c("dark red", "red", "orange", "yellow"))` for colorbar specification in `superheat`.



```{r fig.align="center", fig.height=8, fig.width=5}
types = paste(leukemia_subset$Type[order.dendrogram(leukemia_dend3)])
hc.order = leuk_hc$order

superheat(as.matrix(dis)[hc.order, hc.order], 
          membership.rows = types,
          membership.cols = types,
          left.label.text.size = 2,
          bottom.label.text.size = 2,
          heat.pal = c("dark red", "red", "orange", "yellow"))

```

Based on the heatmap, T-ALL and Hyperdip50 seem to have more in common with one another than other subsets of genes. However, T-ALL and TEL-AML1 seem to have only slightly less in common than the T-ALL and Hyperdip50 pairing of genes based on visual inspection.

__(h).__  (__231 only__). You can also use `superheat` to generate a hierachical clustering dendrogram or a kmeans clustering. First, use `leukemia_subset` to run hierachical clustering and draw the dendrogram. Second, use the same dataset to run kmeans clustering with three the optimal number of clusters, and order the genes (columns) based on hierarchical clustering.

Hint: arguments `row.dendrogram`, `clustering.method`, `n.cluster.rows` and `pretty.order.cols` may be useful, please read the argument descriptions before you attempt the problem. The package manual can be found here: <https://cran.r-project.org/web/packages/superheat/superheat.pdf>




```{r fig.align="center", fig.height=8, fig.width=5}
hc.heatmap = superheat(leukemia_subset[,-1],
          clustering.method = "hierarchical",
          row.dendrogram = T,
          pretty.order.cols = T,
          left.label = "variable",
          left.label.text.size = 2,
          bottom.label.text.size = 2,
          heat.pal = c("dark red", "red", "orange", "yellow"))
```



```{r fig.align="center", fig.height=8, fig.width=5}

set.seed(1)
kmeans.heatmap = superheat(leukemia_subset[,-1],
          scale = T,
          clustering.method = "kmeans",
          n.clusters.rows = 3,
          pretty.order.cols = T, # logical specifying to order the columns based on hierarchical clustering
          left.label = "cluster",
          left.label.text.size = 2,
          bottom.label.text.size = 2,
          heat.pal = c("dark red", "red", "orange", "yellow")
          )
```


